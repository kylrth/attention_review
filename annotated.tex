\documentclass[11pt]{article}

\author{Kyle Roth}

\usepackage[margin=1in]{geometry}
\usepackage{apacite}
\usepackage{etoolbox}
\pretocmd{\bibitem}{\bfseries}{}{}
\pretocmd{\APACrefannotation}{\normalfont}{}{}

\title{Annotated Bibliography}

\begin{document}

\maketitle

In this paper I will discuss the technical applications of attention mechanisms used in neural networks. Attention was first invented as a way to solve long-range dependencies when doing sequence to sequence models, like machine translation. The application of attention has since broadened to include sequence comparison, image generation, image description, and a host of other tasks. The theoretical breadth of attention has also increased, as observed in \cite{attn_all_you_need} and \cite{structured_attention_net}. But there is evidence to suggest that attention is just a weakened version of active memory \cite{active_memory}, meaning that further investigation of attention could bring better approaches to light.

Studying the body of work on the subject of attention mechanisms will help me develop my personal understanding of attention, so that I can implement it myself when creating my own models. This study will give me a greater depth to understand what has been done with attention and what open questions have been left unanswered. Perhaps I will find that attention is now defunct because a newer approach is more versatile. In that case, I want to understand why that is.

\nocite{*}

\bibliographystyle{apacann}
\bibliography{main}

\end{document}
