\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{ijcai19}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tocloft}
\urlstyle{same}

% dot leaders for section entries in table of contents
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

\title{Is attention really all you need?}

\author{
Kyle Roth\\
\affiliations
Brigham Young University\\
\emails
kylrth@gmail.com
}

\begin{document}

\maketitle

\tableofcontents

\vspace{30px}

\begin{abstract}
This is a wonderful abstract.
\end{abstract}

\section{Introduction}

The attention mechanism is an exciting development in the artificial intelligence community. The concept is simple, and rooted in our understanding of attention in biological intelligence \cite{glimpses,neuroscience-inspired}. Attention mechanisms also lend themselves to more intuitive interpretation, an attractive feature as concerns about model interpretability gain traction.

Ironically, great attention has been given to attention mechanisms since their conception in \citeyear{joint_align_translate}. Initially formulated as a word alignment mechanism for text translation \cite{joint_align_translate},

\section{Theoretical generalization}

\subsection{Self-attention}

\subsection{Attention between pairs}

\subsection{Attention on arbitrary graphs}

\subsection{Attention as a lookup}

\subsection{Active memory}

Neural memory resources aren't the same thing \cite{neural_turing}?

\section{Application}

\section{Interpretation}

\bibliographystyle{named}
\bibliography{main}

\end{document}
