@string{ NIPS = {Advances in Neural Information Processing Systems}}

@article{joint_align_translate,
  author = {Dzmitry Bahdanau and
      Kyunghyun Cho and
      Yoshua Bengio},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal = {CoRR},
  volume = {abs/1409.0473},
  year = {2016},
  archivePrefix = {arXiv},
  eprint = {1409.0473},
  url = {https://arxiv.org/pdf/1409.0473.pdf},
  urldate = {Nov 21, 2019},
  annotate = {This paper was foundational to attention mechanisms. Up to this point, RNNs for machine translation relied on an encoding vector of fixed length to represent the entire sentence. This time, the contributors allowed the decoder to rely on encoded representations of each word in the sentence, allowing it to learn the (sometimes long range) dependencies between individual words. This paper coined the phrase ``attention mechanism'', defined it as a learned mechanism (instead of a latent variable), and demonstrated that models with attention achieved higher performance on translations of long sentences. They produced the first neural machine translation model with accuracy comparable to that of statistical models.}
}

@article{attn_all_you_need,
  author = {Ashish Vaswani and
      Noam Shazeer and
      Niki Parmar and
      Jakob Uszkoreit and
      Llion Jones and
      Aidan N. Gomez and
      Lukasz Kaiser and
      Illia Polosukhin},
  title = {Attention Is All You Need},
  editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  journal = NIPS,
  volume = {30},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  pages = {5998--6008},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
  urldate = {Nov 19, 2019},
  annotate = {This work did away with the recurrent layers used with attention up until this point, simply training the attention mechanism to perform the task of text translation. Recurrent models have to be run sequentially, so removing them allows training and evaluation to run faster. They achieved state-of-the-art performance on English-to-German and English-to-French, while being able to train the models in significantly less time. In this work the researchers were careful to analyze what each component of the model contributed, allowing them to draw interesting conclusions. Another contribution is the definition of attention as a learned function of queries, keys, and values. The source of each of these inputs determines the kind of attention used.}
}

@article{listen_attend_spell,
  author = {William Chan and
      Navdeep Jaitly and
      Quoc V. Le and
      Oriol Vinyals},
  title = {Listen, Attend, and Spell},
  journal = {CoRR},
  volume = {abs/1508.01211},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {Nov 21, 2019},
  annotate = {The same year the original attention paper was published, this group performed end-to-end speech transcription with an RNN+attention model. They used a time-dimension reducing BLSTM network to convert raw audio into a compressed representation, and then an RNN with attention to decode that representation into character sequences. This work demonstrated that attention could be used to allow for greater dependence between outputs, because without those dependencies a character-level output would not work. This paper does not contribute in terms of model architecture, but does demonstrate attention's application for character-level decoding.}
}

@article{attention_based_enc_dec,
  author = {KyungHyun Cho and
      Aaron C. Courville and
      Yoshua Bengio},
  title = {Describing Multimedia Content using Attention-based Encoder-Decoder Networks},
  journal = {CoRR},
  volume = {abs/1507.01053},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1507.01053},
  url = {http://arxiv.org/abs/1507.01053},
  urldate = {Nov 21, 2019},
  annotate = {This article offers a valuable introduction to the structure of an attention network, as used in the first paper on attention. It also reviews work on using attention for images and videos, which lends itself to tasks where object identification and location are both important, such as content descriptions. The paper provides a good understand of how convolutions and recurrent layers can both be used in networks with attention mechanisms.}
}

@inproceedings{show_observe_tell,
  author = {Hui Chen and
      Guiguang Ding and
      Zijia Lin and
      Sicheng Zhao and
      Jungong Han},
  title = {Show, Observe and Tell: Attribute-driven Attention Model for Image Captioning},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages = {606--612},
  year = {2018},
  month = {7},
  doi = {10.24963/ijcai.2018/84},
  url = {https://doi.org/10.24963/ijcai.2018/84},
  urldate = {Nov 21, 2019},
  annotate = {This paper implemented a CNN-RNN attention architecture for image captioning, achieving state-of-the-art performance on the task. The attention module received both region-based and attribute-based features from the image. This gave the model the ability to adaptively attend to objects in the image, taking into account the position and attributes of the objects. This paper demonstrates the versatility of the attention mechanism with regard to input feature type.}
}

@article{DBLP:journals/corr/ParikhT0U16,
  author = {Ankur P. Parikh and
      Oscar T{\"{a}}ckstr{\"{o}}m and
      Dipanjan Das and
      Jakob Uszkoreit},
  title = {A Decomposable Attention Model for Natural Language Inference},
  journal = {CoRR},
  volume = {abs/1606.01933},
  year = {2016},
  url = {http://arxiv.org/abs/1606.01933},
  archivePrefix = {arXiv},
  eprint = {1606.01933},
  urldate = {Nov 21, 2019},
  annotate = {Other articles referenced here use attention as a way to describe relationships between elements in a sequence. This paper extends attention to learning the relationship between elements in two sequences, in this case allowing the model to determine whether one sentence entails or contradicts the other. (Intra-sequence attention is also used to provide the model with the ``compositional relationships'' between words in the sentence.) The authors' model achieved accuracy that slightly outperformed state-of-the-art models that were much larger. This interesting use of attention points to further uses of attention beyond what's currently considered.}
}

@article{self_attentive_embedding,
  author = {Zhouhan Lin and
      Minwei Feng and
      C{\'{\i}}cero Nogueira dos Santos and
      Bing Xiang and
  Bowen Zhou and
  Yoshua Bengio},
  title = {A Structured Self-attentive Sentence Embedding},
  journal = {CoRR},
  volume = {abs/1703.03130},
  year = {2017},
  archivePrefix = {arXiv},
  eprint = {1703.03130},
  url = {http://arxiv.org/abs/1703.03130},
  urldate = {Nov 21, 2019},
  annotate = {This work uses attention to create an embedding for sentences that reflects the semantic value of each sentence. In this case, each hidden vector in the LSTM is used as input to a self-attention mechanism, which compiles a single vector that tends to focus on a specific component of the sentence. This use of attention is distinct because it focuses on creating representations that are semantically significant without any further processing (like in translation or transcription).}
}

@article{structured_attention_net,
  author = {Yoon Kim and
      Carl Denton and
      Luong Hoang and
      Alexander M. Rush},
  title = {Structured Attention Networks},
  journal = {CoRR},
  volume = {abs/1702.00887},
  year = {2017},
  archivePrefix = {arXiv},
  eprint = {1702.00887},
  url = {http://arxiv.org/abs/1702.00887},
  urldate = {Nov 21, 2019},
  annotate = {This paper generalizes the idea of attention beyond its application to sequences, like sentences or audio samples, to arbitrarily-structured graphs. Models like conditional random fields can be used to model structural dependencies during end-to-end training of neural networks. Allowing for tree-like structure lets attention models attend to subsequences, and this accomplishes near-state-of-the-art results on machine translation and on natural language inference. This generalization of attention mechanisms deserves further study.}
}

@article{active_memory,
  author    = {Lukasz Kaiser and
               Samy Bengio},
  title     = {Can Active Memory Replace Attention?},
  journal   = {CoRR},
  volume    = {abs/1610.08613},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1610.08613},
  url       = {http://arxiv.org/abs/1610.08613},
  urldate = {Nov 21, 2019},
  annotate = {Soft-decision attention functions much like memory, in that values are stored for future use and read from somewhat selectively. This paper discusses the benefits of using active memory as a replacement for attention, and finds that in nearly all cases attention performs worse than active memory. This has broad implications for the use we find in the attention mechanism, and future work with attention mechanisms will need to determine whether attention's usefulness is found simply in attention as memory.}
}
