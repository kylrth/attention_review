@string{ NIPS = {Advances in Neural Information Processing Systems}}

@article{joint_align_translate,
  author = {Dzmitry Bahdanau and
      Kyunghyun Cho and
      Yoshua Bengio},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal = {CoRR},
  volume = {abs/1409.0473},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1409.0473},
  url = {https://arxiv.org/pdf/1409.0473.pdf},
  urldate = {Nov 21, 2019},
  annotate = {This paper was foundational to attention mechanisms. Up to this point, RNNs for machine translation relied on an encoding vector of fixed length to represent the entire sentence. This time, the contributors allowed the decoder to rely on encoded representations of each word in the sentence, allowing it to learn the (sometimes long range) dependencies between individual words. This paper coined the phrase ``attention mechanism'', defined it as a learned mechanism (instead of a latent variable), and demonstrated that models with attention achieved higher performance on translations of long sentences. They produced the first neural machine translation model with accuracy comparable to that of statistical models.}
}

@article{attn_all_you_need,
  author = {Ashish Vaswani and
      Noam Shazeer and
      Niki Parmar and
      Jakob Uszkoreit and
      Llion Jones and
      Aidan N. Gomez and
      Lukasz Kaiser and
      Illia Polosukhin},
  title = {Attention Is All You Need},
  editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  journal = NIPS,
  volume = {30},
  year = {2017},
  publisher = {Curran Associates, Inc.},
  pages = {5998--6008},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
  urldate = {Nov 19, 2019},
  annotate = {This work did away with the recurrent layers used with attention up until this point, simply training the attention mechanism to perform the task of text translation. Recurrent models have to be run sequentially, so removing them allows training and evaluation to run faster. They achieved state-of-the-art performance on English-to-German and English-to-French, while being able to train the models in significantly less time. In this work the researchers were careful to analyze what each component of the model contributed, allowing them to draw interesting conclusions. Another contribution is the definition of attention as a learned function of queries, keys, and values. The source of each of these inputs determines the kind of attention used.}
}

@article{listen_attend_spell,
  author = {William Chan and
      Navdeep Jaitly and
      Quoc V. Le and
      Oriol Vinyals},
  title = {Listen, Attend, and Spell},
  journal = {CoRR},
  volume = {abs/1508.01211},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1508.01211},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {Nov 21, 2019},
  annotate = {The same year the original attention paper was published, this group performed end-to-end speech transcription with an RNN+attention model. They used a time-dimension reducing BLSTM network to convert raw audio into a compressed representation, and then an RNN with attention to decode that representation into character sequences. This work demonstrated that attention could be used to allow for greater dependence between outputs, because without those dependencies a character-level output would not work. This paper does not contribute in terms of model architecture, but does demonstrate attention's application for character-level decoding.}
}

@article{attention_based_enc_dec,
  author = {KyungHyun Cho and
      Aaron C. Courville and
      Yoshua Bengio},
  title = {Describing Multimedia Content using Attention-based Encoder-Decoder Networks},
  journal = {CoRR},
  volume = {abs/1507.01053},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1507.01053},
  url = {http://arxiv.org/abs/1507.01053},
  urldate = {Nov 21, 2019},
  annotate = {This article offers a valuable introduction to the structure of an attention network, as used in the first paper on attention. It also reviews work on using attention for images and videos, which lends itself to tasks where object identification and location are both important, such as content descriptions. The paper provides a good understanding of how convolutions and recurrent layers can both be used in networks with attention mechanisms.}
}

@inproceedings{show_observe_tell,
  author = {Hui Chen and
      Guiguang Ding and
      Zijia Lin and
      Sicheng Zhao and
      Jungong Han},
  title = {Show, Observe and Tell: Attribute-driven Attention Model for Image Captioning},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages = {606--612},
  year = {2018},
  month = {7},
  doi = {10.24963/ijcai.2018/84},
  url = {https://doi.org/10.24963/ijcai.2018/84},
  urldate = {Nov 21, 2019},
  annotate = {This paper implemented a CNN-RNN attention architecture for image captioning, achieving state-of-the-art performance on the task. The attention module received both region-based and attribute-based features from the image. This gave the model the ability to adaptively attend to objects in the image, taking into account the position and attributes of the objects. This paper demonstrates the versatility of the attention mechanism with regard to input feature type.}
}

@article{natural_language_inference,
author = {Ankur P. Parikh and
Oscar T{\"{a}}ckstr{\"{o}}m and
Dipanjan Das and
Jakob Uszkoreit},
title = {A Decomposable Attention Model for Natural Language Inference},
journal = {CoRR},
volume = {abs/1606.01933},
year = {2016},
url = {http://arxiv.org/abs/1606.01933},
archivePrefix = {arXiv},
eprint = {1606.01933},
urldate = {Nov 21, 2019},
annotate = {Other articles referenced here use attention as a way to describe relationships between elements in a sequence. This paper extends attention to learning the relationship between elements in two sequences, in this case allowing the model to determine whether one sentence entails or contradicts the other. (Intra-sequence attention is also used to provide the model with the ``compositional relationships'' between words in the sentence.) The authors' model achieved accuracy that slightly outperformed state-of-the-art models that were much larger. This interesting use of attention points to further uses of attention beyond what's currently considered.}
}

@article{self_attentive_embedding,
author = {Zhouhan Lin and
Minwei Feng and
C{\'{\i}}cero Nogueira dos Santos and
Bing Xiang and
Bowen Zhou and
Yoshua Bengio},
title = {A Structured Self-attentive Sentence Embedding},
journal = {CoRR},
volume = {abs/1703.03130},
year = {2017},
archivePrefix = {arXiv},
eprint = {1703.03130},
url = {http://arxiv.org/abs/1703.03130},
urldate = {Nov 21, 2019},
annotate = {This work uses attention to create an embedding for sentences that reflects the semantic value of each sentence. In this case, each hidden vector in the LSTM is used as input to a self-attention mechanism, which compiles a single vector that tends to focus on a specific component of the sentence. This use of attention is distinct because it focuses on creating representations that are semantically significant without any further processing (like in translation or transcription).}
}

@article{graphs,
  author = {Yoon Kim and
      Carl Denton and
      Luong Hoang and
      Alexander M. Rush},
  title = {Structured Attention Networks},
  journal = {CoRR},
  volume = {abs/1702.00887},
  year = {2017},
  archivePrefix = {arXiv},
  eprint = {1702.00887},
  url = {http://arxiv.org/abs/1702.00887},
  urldate = {Nov 21, 2019},
  annotate = {This paper generalizes the idea of attention beyond its application to sequences, like sentences or audio samples, to arbitrarily-structured graphs. Models like conditional random fields can be used to model structural dependencies during end-to-end training of neural networks. Allowing for tree-like structure lets attention models attend to subsequences, and this accomplishes near-state-of-the-art results on machine translation and on natural language inference. This generalization of attention mechanisms deserves further study.}
}

@misc{wiki:turing,
   author = "Wikipedia",
   title = "Turing machine --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2019",
   url = "\url{https://en.wikipedia.org/w/index.php?title=Turing_machine}",
   note = "[Online; accessed 13-Dec-2019]"
 }

@article{active_memory,
  author    = {Lukasz Kaiser and
      Samy Bengio},
  title     = {Can Active Memory Replace Attention?},
  journal   = {CoRR},
  volume    = {abs/1610.08613},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1610.08613},
  url       = {http://arxiv.org/abs/1610.08613},
  urldate = {Nov 21, 2019},
  annotate = {Soft-decision attention functions much like memory, in that values are stored for future use and read from somewhat selectively. This paper discusses the benefits of using active memory as a replacement for attention, and finds that in nearly all cases attention performs worse than active memory. This has broad implications for the use we find in the attention mechanism, and future work with attention mechanisms will need to determine whether attention's usefulness is found simply in attention as memory.}
}

@article{draw,
  author = {Karol Gregor and
      Ivo Danihelka and
      Alex Graves and
      Daan Wierstra},
  title = {{DRAW:} {A} Recurrent Neural Network For Image Generation},
  journal = {CoRR},
  volume = {abs/1502.04623},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1502.04623},
  url = {http://arxiv.org/abs/1502.04623},
  urldate = {Nov 21, 2019},
  annotate = {These authors combine variational autoencoders (VAEs) with recurrent networks and attention to create a model that iteratively improves a generated image from an encoded input vector. At each iteration, attention is used both to choose which part of the encoding to consider, and to choose which part of the output drawing to update. This interesting application of attention is very natural to the way humans understand image generation, and also presents promising models for image classification.}
}

@inproceedings{glimpses,
  author = {Hugo Larochelle and
      Geoffrey Hinton},
  title = {Learning to Combine Foveal Glimpses with a Third-order Boltzmann Machine},
  booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
  series = {NIPS'10},
  year = {2010},
  location = {Vancouver, British Columbia, Canada},
  pages = {1243--1251},
  numpages = {9},
  url = {http://dl.acm.org/citation.cfm?id=2997189.2997328},
  acmid = {2997328},
  publisher = {Curran Associates Inc.},
  address = {USA},
}

@article{neuroscience-inspired,
  author = {Demis Hassabis and
      Dharshan Kumaran and
      Christopher Summerfield and
      Matthew Botvinick},
  title = {Neuroscience-Inspired Artificial Intelligence},
  journal = {Neuron},
  volume = {95},
  issue = {2},
  year = {2017},
  publisher = {Cell Press},
  pages = {245--258},
  url = {https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3},
  urldate = {Dec 4, 2019}
}

@article{neural_turing,
  title={Neural {T}uring Machines},
  author={Alex Graves and Greg Wayne and Ivo Danihelka},
  journal={CoRR},
  year={2014},
  volume={abs/1410.5401},
  archivePrefix = {arXiv},
  eprint = {1410.5401},
  url = {http://arxiv.org/abs/1410.5401},
  urldate = {Dec 4, 2019},
}

@article{attention_please,
  author    = {Andrea Galassi and
               Marco Lippi and
               Paolo Torroni},
  title     = {Attention, please! {A} Critical Review of Neural Attention Models in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1902.02181},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02181},
  archivePrefix = {arXiv},
  eprint    = {1902.02181},
  urldate = {Dec 5, 2019}
}

@article{gradient_descent,
  author    = {Sebastian Ruder},
  title     = {An overview of gradient descent optimization algorithms},
  journal   = {CoRR},
  volume    = {abs/1609.04747},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04747},
  archivePrefix = {arXiv},
  eprint    = {1609.04747},
  urldate = {Dec 11, 2019}
}

@article{universal_approximators,
  author="George Cybenko",
  title="Approximation by superpositions of a sigmoidal function",
  journal="Mathematics of Control, Signals and Systems",
  year="1989",
  month="Dec",
  day="01",
  volume="2",
  number="4",
  pages="303--314",
  abstract="In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.",
  issn="1435-568X",
  doi="10.1007/BF02551274",
  url="https://doi.org/10.1007/BF02551274"
}

@inproceedings{relu,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Vinod Nair and Geoffrey E Hinton},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@online{backprop,
  author = {Guru99},
  year = {2019},
  title = {Back Propagation Neural Network: Explained With Simple Example},
  howpublished = {\url{https://www.guru99.com/backpropogation-neural-network.html}},
  urldate = {2019-12-05}
}

@inproceedings{encoder_decoders,
  title = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  author = {Kyunghyun Cho and
      Bart van Merri{\"e}nboer and
      Dzmitry Bahdanau and
      Yoshua Bengio},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax,Semantics and Structure in Statistical Translation},
  month = {oct},
  year = {2014},
  address = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url = {https://www.aclweb.org/anthology/W14-4012},
  doi = {10.3115/v1/W14-4012},
  pages = {103--111}
}

@article{bi_rnn,
  title={Bidirectional recurrent neural networks},
  author={Mike Schuster and Kuldip K. Paliwal},
  journal={IEEE Trans. Signal Processing},
  year={1997},
  volume={45},
  pages={2673-2681}
}

@book{backprop_theory,
  title={Theory of the backpropagation neural network},
  author={Robert Hecht-Nielsen},
  booktitle={Neural networks for perception},
  pages={65--93},
  year={1992},
  publisher={Elsevier}
}
